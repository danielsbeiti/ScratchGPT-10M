{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# **Scratch GPT**\n",
    "\n",
    "---\n",
    "\n",
    "## Daniel Sbeiti\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  5407k      0 --:--:-- --:--:-- --:--:-- 5419k\n"
     ]
    }
   ],
   "source": [
    "!curl -o input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 500 characters of the dataset:\n",
      "@BillyM2k I find the gold toe sock – inevitably off kilter &amp; washed out – a little troubling esthetically &amp; arguably a bit corpo\n",
      "\n",
      "Sock Con, the conference for socks\n",
      "\n",
      "Always something new for the magazine cover and the articles practically write themselves\n",
      "\n",
      "@ExplainThisBob This guy gets it\n",
      "\n",
      "Sock tech is so advanced that you can get pretty much anything in sock form these days!\n",
      "\n",
      "I must confess to a penchant for creative socks\n",
      "\n",
      "@slashdot It’s time\n",
      "\n",
      "@TonyadeVitti @historydefined His success was in fact due, in part, because he was super fun at parties, spoke and wrote incredibly well!\n",
      "\n",
      "@historydefined While bleak posts maybe generate more clicks, more happier moments in history would be nice\n",
      "\n",
      "@mishaboar @boringcompany Supporting Doge wherever possible\n",
      "\n",
      "Moon brought us together in ‘69,\n",
      "Mars can do that in the future\n",
      "\n",
      "Without a common goal, \n",
      "humanity will fight itself\n",
      "\n",
      "@PPathole Exactly\n",
      "\n",
      "@AlexBerenson @Twitter Can you say more about this: “… pressures that the government may have pla\n"
     ]
    }
   ],
   "source": [
    "with open('elonx.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "print('First 500 characters of the dataset:')\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Character Level Vocabulary** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 414840\n",
      "Number of unique characters: 218\n",
      "Unique characters: \n",
      " !\"#$%&'()*+,-./0123456789:;?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz{|}~ àéō‍–‘’“”…≠☺♂♥⚡❤⬇始皇秦️🇦🇪🇫🇮🇯🇲🇳🇴🇵🇷🇸🇺🌶🌹🍀🍆🍩🍳🍷🍻🍿🎂🎣🎯🎱🎶🏆🐈🐝🐥🐰🐹👀👅👏👑👖👶👽💕💗💙💛💥💦💨💩💪💫💯💰📜🔁🔥🖤😀😂😃😈😉😊😋😎😑😔😜😢😭😮😳😴🙄🙌🙏🚀🚃🚽🛌🛰🛸🤔🤖🤗🤞🤣🤦🤩🤭🤯🤷🥁🥜🥰🥲🥹🦇🧀🧌🧐🧠🧵🩸🪶🫐🫡🫶\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print('Length of dataset in characters:', len(text))\n",
    "print('Number of unique characters:', vocab_size)\n",
    "print('Unique characters:', ''.join(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Encoder-Decooder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of characters to indices: {'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "\n",
      "Encoding example:\n",
      "[20, 43, 50, 50, 53, 6, 1, 35, 53, 56, 50, 42, 2]\n",
      "\n",
      "Decoding example:\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "## Mapping characters to integers and vice versa\n",
    "stoi = { ch: i for i, ch in enumerate(chars) }\n",
    "itos = { i: ch for i, ch in enumerate(chars) }\n",
    "print('Mapping of characters to indices:', stoi)\n",
    "\n",
    "\n",
    "# Encode the text into integers\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "# Decode the integers back into text\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "print('\\nEncoding example:')\n",
    "print(encode('Hello, World!'))\n",
    "print('\\nDecoding example:')\n",
    "print(decode(encode('Hello, World!')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **We encode dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([1115394])\n",
      "Data type: torch.int64\n",
      "\n",
      "First 100 characters of encoded data:\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print('Data shape:', data.shape)\n",
    "print('Data type:', data.dtype)\n",
    "print('\\nFirst 100 characters of encoded data:')\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Train Test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([1003854])\n",
      "Validation data shape: torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print('Training data shape:', train_data.shape)\n",
    "print('Validation data shape:', val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Context concept**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of context usage:\n",
      "Input: tensor([18]), Target: 47\n",
      "Input: tensor([18, 47]), Target: 56\n",
      "Input: tensor([18, 47, 56]), Target: 57\n",
      "Input: tensor([18, 47, 56, 57]), Target: 58\n",
      "Input: tensor([18, 47, 56, 57, 58]), Target: 1\n",
      "Input: tensor([18, 47, 56, 57, 58,  1]), Target: 15\n",
      "Input: tensor([18, 47, 56, 57, 58,  1, 15]), Target: 47\n",
      "Input: tensor([18, 47, 56, 57, 58,  1, 15, 47]), Target: 58\n"
     ]
    }
   ],
   "source": [
    "## Defining block as context size\n",
    "block_size = 8\n",
    "\n",
    "## Example of context usage in training\n",
    "print('Example of context usage:')\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for i in range(block_size):\n",
    "    print(f'Input: {x[:i+1]}, Target: {y[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a batch:\n",
      "\n",
      "Batch x shape: torch.Size([4, 8])\n",
      "Batch y shape: torch.Size([4, 8])\n",
      "\n",
      "Batch x of 4 independent sequences:\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "Batch y associated:\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4   ## batch sequences to be processed in parallel, we split text in 4\n",
    "block_size = 8   ## context size to look at in the past\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # random starting indices\n",
    "    \n",
    "    # Create batches of data\n",
    "    # x is the input, y is the target\n",
    "    # x is the sequence of characters, y is the next character for each\n",
    "    # x[i] is the input sequence, y[i] is the target sequence\n",
    "\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Example of getting a batch\n",
    "print('Example of a batch:')\n",
    "xb, yb = get_batch('train')\n",
    "print('\\nBatch x shape:', xb.shape)\n",
    "print('Batch y shape:', yb.shape)\n",
    "print('\\nBatch x of 4 independent sequences:')\n",
    "print(xb)\n",
    "print('\\nBatch y associated:')\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Bigram LM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example forward pass:\n",
      "Output shape: torch.Size([32, 65])\n",
      "Loss value: 5.036386013031006\n",
      "\n",
      "Example generation:\n",
      "Input sequence: \n",
      "\n",
      "Generated sequence:\n",
      "\n",
      "lfJeukRuaRJKXAYtXzfJ:HEPiu--sDioi;ILCo3pHNTmDwJsfheKRxZCFs\n",
      "lZJ XQc?:s:HEzEnXalEPklcPU cL'DpdLCafBheH\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        ## Embedding layer\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        ## Forward pass, we get scores for next characters of sequence given the input for each vocab\n",
    "        logits = self.token_embedding_table(idx)   # (batch_size (B), block_size (T), vocab_size (C))\n",
    "\n",
    "        ## Loss to know how good logits generated are close to targets\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            ### Reshape logits and targets to compute loss if targets are provided\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            ### Compute loss cross entropy\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        ## Generate new tokens for every B dimension (batch) over time (T) for max_new_tokens\n",
    "        ## idx is the input sequence, we will append new tokens to it\n",
    "        ## idx shape: (B, T)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            ### get the logits for the next character\n",
    "            logits, _ = self(idx)       # logits shape: (B, T, C)\n",
    "\n",
    "            ### Focus on the last time step\n",
    "            logits = logits[:, -1, :]   # logits shape: (B, C)\n",
    "\n",
    "            ### Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # probs shape: (B, C)\n",
    "\n",
    "            ### Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # idx_next shape: (B, 1) (1 pred for each batch)\n",
    "\n",
    "            ### Append the new token to the input sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "\n",
    "\n",
    "### Example\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "print('\\nExample forward pass:')\n",
    "out, loss = model(xb, yb)\n",
    "\n",
    "print('Output shape:', out.shape)\n",
    "print('Loss value:', loss.item())\n",
    "\n",
    "### Example of generation, we give it a 0 which is coding for new line\n",
    "print('\\nExample generation:')\n",
    "print('Input sequence:', decode([0]))\n",
    "print('Generated sequence:')\n",
    "print(decode(model.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Training of model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 4.647705078125\n",
      "Step 1000, Loss: 3.664973020553589\n",
      "Step 2000, Loss: 3.3166022300720215\n",
      "Step 3000, Loss: 2.8780226707458496\n",
      "Step 4000, Loss: 2.6928253173828125\n",
      "Step 5000, Loss: 2.4840328693389893\n",
      "Step 6000, Loss: 2.504540205001831\n",
      "Step 7000, Loss: 2.510176658630371\n",
      "Step 8000, Loss: 2.409280776977539\n",
      "Step 9000, Loss: 2.4302585124969482\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for step in range(10000):\n",
    "    xb, yb = get_batch('train')      # get a batch of data\n",
    "    logits, loss = model(xb, yb)     # forward pass, get logitys and loss\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)   # zero the gradients\n",
    "    loss.backward()                         # backward pass, compute gradients\n",
    "    optimizer.step()                        # update the weights\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f'Step {step}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final generation:\n",
      "Input sequence: \n",
      "\n",
      "Generated sequence:\n",
      "\n",
      "M:\n",
      "IUSh t,\n",
      "F th he d ke alved.\n",
      "Thupld, cipbll t\n",
      "I: ir w, l me sie hend lor ito'l an e\n",
      "\n",
      "I:\n",
      "Gochosen ea ar btamandd halind\n",
      "Aust, plt t wadyotl\n",
      "I bel qunganonoth he m he de avellis k'l, tond soran:\n",
      "\n",
      "WI he toust are bot g e n t s d je hid t his IAces I my ig t\n",
      "Ril'swoll e pupat inouleacends-athiqu heame\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\nFinal generation:')\n",
    "print('Input sequence:', decode([0]))\n",
    "print('Generated sequence:')\n",
    "print(decode(model.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Self Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Masked weights example batch:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2174, 0.7826, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0876, 0.8928, 0.0195, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0564, 0.7278, 0.0292, 0.1866, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2825, 0.1124, 0.0626, 0.5385, 0.0040, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0052, 0.4463, 0.0116, 0.2922, 0.0121, 0.2326, 0.0000, 0.0000],\n",
      "        [0.6334, 0.0123, 0.0444, 0.1499, 0.0048, 0.1514, 0.0038, 0.0000],\n",
      "        [0.0271, 0.1895, 0.0454, 0.0469, 0.1161, 0.0911, 0.0329, 0.4510]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, vocab_size\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "\n",
    "### One head of SA\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)   # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v = value(x) # (B, T, head_size)\n",
    "\n",
    "### Attention scores\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "\n",
    "\n",
    "### Attention mask\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print('\\nMasked weights example batch:')\n",
    "print(wei[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of one head of self-attention:\n",
      "torch.Size([4, 8, 16])\n",
      "\n",
      "Output of one head of self-attention for first batch:\n",
      "tensor([[-8.2172e-01,  4.7129e-01,  5.8801e-02, -4.4999e-01,  5.2736e-01,\n",
      "         -8.4573e-01,  1.4697e-04, -4.8940e-01, -4.9378e-01, -2.8120e-01,\n",
      "         -5.4854e-01, -1.0617e-01,  1.4664e+00, -1.9969e-01,  4.4788e-01,\n",
      "         -3.1392e-01],\n",
      "        [-4.0749e-01,  8.1118e-02,  4.6230e-01,  4.9075e-01,  1.3289e-01,\n",
      "          6.8089e-01,  3.6334e-01,  3.1203e-01,  2.2027e-01, -3.6878e-01,\n",
      "         -1.6352e-01,  3.0710e-01,  7.1388e-01, -2.2744e-01, -2.9124e-02,\n",
      "         -7.3181e-01],\n",
      "        [-3.3474e-01,  4.2210e-02,  5.1649e-01,  6.0874e-01,  8.0613e-02,\n",
      "          9.1590e-01,  4.2155e-01,  4.3073e-01,  3.3944e-01, -3.7708e-01,\n",
      "         -8.2194e-02,  3.6995e-01,  5.8980e-01, -2.2084e-01, -1.1997e-01,\n",
      "         -8.0739e-01],\n",
      "        [-2.2626e-01,  5.5309e-02,  4.3217e-01,  5.8852e-01,  6.1528e-02,\n",
      "          7.6804e-01,  4.0485e-01,  4.8535e-01,  3.0600e-01, -1.1146e-01,\n",
      "         -2.1775e-01,  2.4776e-01,  5.0417e-01, -2.1021e-01,  2.3954e-02,\n",
      "         -8.0983e-01],\n",
      "        [-1.6727e-01,  2.4507e-01,  1.1307e-01,  1.7704e-01,  1.7945e-01,\n",
      "         -8.6460e-02,  2.3767e-01,  2.7900e-01, -1.6327e-02,  4.2805e-01,\n",
      "         -5.9900e-01, -1.4434e-01,  6.1684e-01, -1.7132e-01,  4.6106e-01,\n",
      "         -6.6470e-01],\n",
      "        [-2.0934e-01,  1.2700e-01,  2.1567e-01,  6.2509e-01, -2.9039e-01,\n",
      "          6.2266e-01,  2.3505e-01,  4.4615e-01,  3.7161e-01,  2.6017e-01,\n",
      "         -2.8974e-01,  8.6986e-02,  3.8561e-01, -2.6268e-01,  1.7121e-01,\n",
      "         -6.2751e-01],\n",
      "        [-5.8265e-01,  4.3443e-01,  1.2898e-02, -1.5221e-01,  1.6465e-01,\n",
      "         -4.3170e-01,  2.6095e-02, -2.0220e-01, -1.7353e-01,  5.4353e-02,\n",
      "         -4.5348e-01, -1.0812e-01,  1.0485e+00, -2.0821e-01,  3.7924e-01,\n",
      "         -3.8470e-01],\n",
      "        [-1.6550e-01,  3.0936e-01,  1.3439e-01, -2.4209e-01, -9.4069e-02,\n",
      "          2.3793e-01,  1.4346e-01,  3.1387e-02,  3.6107e-01,  7.3853e-02,\n",
      "          1.3078e-01,  9.2046e-02,  5.6289e-01, -5.2302e-02, -2.7322e-01,\n",
      "         -1.7075e-01]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = wei @ v\n",
    "\n",
    "print('\\nOutput of one head of self-attention:')\n",
    "print(out.shape)\n",
    "print(\"\\nOutput of one head of self-attention for first batch:\")\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
